{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiQC in Terra\n",
    "\n",
    "This notebook will search through the submissions in a Terra workspace for QC files, download them to the vm, run [MultiQC](https://multiqc.info/), upload the resulting reports into the google bucket associated with this workspace, and provide a link to those reports.  Workflows will be grouped according to workflow name, and a different report will be generated for each workflow name.\n",
    "\n",
    "This notebook must be run on a VM which has included the starup script `gs://broad-dsde-methods-ckachulis/MultiQC_in_Terra/multiqc_terra_startup.sh`.  If you don't have access to that file, you can use your own by uploading the following as a bash script into a google bucket of your choice.\n",
    "```shell\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "pip install git+https://github.com/kachulis/MultiQC.git@ck_gcp\n",
    "```\n",
    "\n",
    "If you can't or really don't want to use a startup script, you can uncomment the next cell, and run it instead.  Note that this cell will kill the kernel and cause it to restart.  This is not a bug, and is necessary for annoying package version reasons, and can be avoided by using the startup script instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# !pip install git+https://github.com/kachulis/MultiQC.git@ck_gcp\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the workspace you want to run multiqc on (by default it will run on this workspace), and any filters in the next cell.  Note that filters are cumulative.  If a workflow fails any of the filters, it will not be included.  A particular filter can be turned off by setting it to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firecloud.api as fapi\n",
    "import os\n",
    "namespace = os.environ.get(\"WORKSPACE_NAMESPACE\")\n",
    "workspace = os.environ.get(\"WORKSPACE_NAME\")\n",
    "\n",
    "submission_ids_to_include = None\n",
    "worklow_ids_to_include = None\n",
    "workflow_names_to_include = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will find the appropriate QC files, download them, run MultiQC, and upload the resulting reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "from multiqc.utils import report, config\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pathlib\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "\n",
    "config.mqc_load_userconfig()\n",
    "\n",
    "def flat(pool):\n",
    "    res = []\n",
    "    for v in pool:\n",
    "        if isinstance(v, list):\n",
    "            res += flat(v)\n",
    "        elif isinstance(v, str) or isinstance(v, tuple):\n",
    "                res.append(v)\n",
    "        else:\n",
    "            raise RuntimeError(f'Trying to flatten and found object of unexpected type {type(v)}: {v}')\n",
    "    return res  \n",
    "\n",
    "#get submissions\n",
    "print(\"Finding submissions...\")\n",
    "submissions=fapi.list_submissions(namespace, workspace).json()\n",
    "submission_ids = [submission['submissionId'] for submission in submissions]\n",
    "if submission_ids_to_include is not None:\n",
    "    submission_ids = [sid for sid in submission_ids if sid in submission_ids_to_include]\n",
    "print(f'Found {len(submission_ids)} submissions.')\n",
    "\n",
    "#get list of tuples of workflow_ids that have succeeded and submission_ids\n",
    "print(\"Finding workflows...\")\n",
    "def get_workflow_ids_for_submission(submission_id):\n",
    "    submission = fapi.get_submission(namespace, workspace, submission_id).json()\n",
    "    workflows = fapi.get_submission(namespace, workspace, submission_id).json()['workflows']\n",
    "    workflow_ids = [(submission_id, workflow['workflowId'], submission['submissionDate']) for workflow in workflows if workflow['status'] == 'Succeeded']\n",
    "    return workflow_ids\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.run_in_executor(executor, get_workflow_ids_for_submission, submission_id) for submission_id in submission_ids]\n",
    "    workflow_with_submissions = [id_tuple for id_tuples in await asyncio.gather(*tasks) for id_tuple in id_tuples]\n",
    "\n",
    "if worklow_ids_to_include is not None:\n",
    "    workflow_with_submissions = [e for e in workflow_with_submissions if e[1] in worklow_ids_to_include]\n",
    "print(f'Found {len(workflow_with_submissions)} workflows.')\n",
    "\n",
    "\n",
    "# build dictionary of outputs for specified worklows\n",
    "print(\"Finding outputs...\")\n",
    "def get_outputs(submission_id, workflow_id, submission_date):\n",
    "    metadata = fapi.get_workflow_metadata(namespace, workspace, submission_id, workflow_id).json()\n",
    "    res = {k:(v, submission_date) for k,v in metadata['outputs'].items() if isinstance(v, list) or isinstance(v, str) and v.startswith(\"gs://\")}\n",
    "#     res = dict(filter(outputs_filter, metadata['outputs'].items()))\n",
    "    return res\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.run_in_executor(executor, get_outputs, submission_id, workflow_id, submission_date) for \n",
    "             submission_id, workflow_id, submission_date in workflow_with_submissions]\n",
    "    outputs_dict = defaultdict(list)\n",
    "    for d in await asyncio.gather(*tasks):\n",
    "        for key, value in d.items():\n",
    "            outputs_dict[key].append(value)\n",
    "\n",
    "outputs_dict = {k: flat(v) for k,v in outputs_dict.items()}\n",
    "if workflow_names_to_include is not None:\n",
    "    outputs_dict = {k:v for k,v in outputs_dict.items() if k.split(\".\")[0] in workflow_names_to_include}\n",
    "print(f'Found {sum([len(v) for v in outputs_dict.values()])} outputs')\n",
    "\n",
    "# for each output key, check whether the output files are useful for multiqc\n",
    "print(\"Checking outputs against MultiQC\")\n",
    "def check_output(key, v):\n",
    "    is_found_multiqc = report.search_gcs(v)\n",
    "    return key, is_found_multiqc\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.run_in_executor(executor, check_output, output_name, output_list[0][0]) for \n",
    "             output_name, output_list in outputs_dict.items()]\n",
    "    status_dict = {k:v for k,v in await asyncio.gather(*tasks)}\n",
    "metrics_file_paths = flat([(k.split(\".\")[0], v2[0], v2[1]) for k,v in outputs_dict.items() if status_dict[k] for v2 in v])\n",
    "print(f'Found {len(metrics_file_paths)} metrics files.')\n",
    "\n",
    "\n",
    "workflow_names = {wn for wn, _, _ in metrics_file_paths}\n",
    "now = datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d_%H:%M:%S_%Z\")\n",
    "for name in workflow_names:\n",
    "    pathlib.Path(f'{name}_{now}_multiqc').mkdir(exist_ok=True)\n",
    "    \n",
    "print(f'Workflows found: {workflow_names}')\n",
    "\n",
    "# remove duplicates\n",
    "print('Removing duplicates/reruns...')\n",
    "metrics_file_groups = defaultdict(list)\n",
    "for wn, on, st in metrics_file_paths:\n",
    "    metrics_file_groups[(wn, os.path.basename(on))].append((wn, on, st))\n",
    "\n",
    "unique_metrics_files=[sorted(v,key=lambda t : datetime.strptime(t[2].split(\".\")[0],\n",
    "                                       \"%Y-%m-%dT%H:%M:%S\"),\n",
    "          reverse=True)[0] for v in metrics_file_groups.values()]\n",
    "print(f'{len(metrics_file_paths)-len(unique_metrics_files)} duplicate/reruns removed, {len(unique_metrics_files)} unique metrics files remain.')\n",
    "\n",
    "#download metrics files\n",
    "print(\"Downloading metrics files...\")\n",
    "def get_bucket_and_blob(uri):\n",
    "    return uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "def download_file(workflow_name, uri):\n",
    "    bucket_name, blob_name = get_bucket_and_blob(uri)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    destination_name = f'{workflow_name}_{now}_multiqc/{blob_name.split(\"/\")[-1]}'\n",
    "    blob.download_to_filename(destination_name)\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [loop.run_in_executor(executor, download_file, workflow_name, uri) for \n",
    "             workflow_name, uri, _ in unique_metrics_files]\n",
    "    await asyncio.gather(*tasks)\n",
    "print(f'{len(unique_metrics_files)} metrics files downloaded')\n",
    "\n",
    "\n",
    "workspace_bucket_name = os.environ.get(\"WORKSPACE_BUCKET\").replace(\"gs://\", \"\")\n",
    "workspace_bucket = storage_client.bucket(workspace_bucket_name)\n",
    "for name in workflow_names:\n",
    "    directory = f'{name}_{now}_multiqc'\n",
    "    report_name = f'{name}_{now}_multiqc.html'\n",
    "    cmd =f'multiqc --filename {report_name} {directory}'\n",
    "    !{cmd}\n",
    "    print(f'Created report {report_name}')\n",
    "    blob=workspace_bucket.blob(f'multiqc_reports/{report_name}')\n",
    "    blob.upload_from_filename(report_name)\n",
    "    print(f'Report has been uploaded to https://console.cloud.google.com/storage/browser/_details/{workspace_bucket_name}/multiqc_reports/{report_name}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
